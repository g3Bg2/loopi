# Ollama Agentic Support - Quick Reference Card

## âš¡ Quick Start (30 seconds)

```bash
# 1. Start Ollama server
ollama serve

# 2. In another terminal, pull a model
ollama pull mistral

# 3. In Loopi UI:
# - Add AI Agent step
# - Provider: Ollama
# - Model: mistral
# - Goal: "Navigate to google.com and take a screenshot"
# - Click Execute!
```

---

## ğŸ“‹ Configuration Template

```json
{
  "type": "aiAgent",
  "provider": "ollama",
  "model": "mistral",
  "goal": "Your automation goal here",
  "temperature": 0,
  "maxTokens": 2048,
  "baseUrl": "http://localhost:11434",
  "allowedSteps": ["navigate", "extract", "click"],
  "storeKey": "result"
}
```

---

## ğŸ¯ Model Quick Guide

| Model | Speed | Quality | Best For | Size |
|-------|-------|---------|----------|------|
| **mistral** | âš¡âš¡âš¡ | â­â­â­ | Fast scraping | 7B |
| **neural-chat** | âš¡âš¡ | â­â­â­â­ | Balanced tasks | 13B |
| **dolphin-mixtral** | âš¡ | â­â­â­â­â­ | Complex analysis | 46B |

### Pull Models:
```bash
ollama pull mistral           # Fast & reliable
ollama pull neural-chat        # Balanced
ollama pull dolphin-mixtral    # Most powerful
```

---

## ğŸ”§ Common Use Cases

### Web Scraping
```json
{
  "model": "mistral",
  "goal": "Extract all product titles and prices",
  "allowedSteps": ["navigate", "extract"]
}
```

### API Testing
```json
{
  "model": "neural-chat",
  "goal": "Call API, validate response, store results",
  "allowedSteps": ["apiCall", "extract", "setVariable"]
}
```

### Complex Workflows
```json
{
  "model": "dolphin-mixtral",
  "goal": "Login, navigate, extract, analyze, notify",
  "maxTokens": 3000,
  "allowedSteps": ["navigate", "click", "type", "extract", "apiCall"]
}
```

---

## ğŸ› ï¸ Troubleshooting

### âŒ "Ollama connection refused"
```bash
# Make sure Ollama is running
ollama serve

# Check it's accessible
curl http://localhost:11434/api/status
```

### âŒ "Model doesn't call tools"
```json
{
  "model": "mistral",  // Use mistral instead
  "goal": "Click the button",  // More specific goal
  "maxTokens": 2048  // Give more space
}
```

### âŒ "Agent loops infinitely"
```json
{
  "goal": "Extract product price",  // Simpler goal
  "allowedSteps": ["extract"],  // Fewer tools
  "maxTokens": 1024  // Limit size
}
```

### âŒ "Timeout errors"
```json
{
  "timeoutMs": 60000,  // Increase timeout
  "model": "mistral"   // Use faster model
}
```

---

## ğŸ“Š Hardware Requirements

```
Minimal (4GB RAM):
  â”œâ”€ mistral 7B âœ…
  â””â”€ Limited to fast scraping

Standard (8GB RAM):
  â”œâ”€ mistral 7B âœ…âœ…
  â”œâ”€ neural-chat 13B âœ…
  â””â”€ Good for most tasks

High-End (16GB+ RAM):
  â”œâ”€ neural-chat 13B âœ…âœ…
  â”œâ”€ dolphin-mixtral 46B âœ…
  â””â”€ Everything supported
```

---

## ğŸš€ Available Tools (12 Total)

**Browser Tools**:
- `navigate` - Go to URL
- `click` - Click element
- `type` - Type text
- `extract` - Get element content
- `screenshot` - Capture page

**Data Tools**:
- `setVariable` - Store value
- `getVariable` - Retrieve value
- `wait` - Pause N seconds

**API Tools**:
- `apiCall` - Make HTTP request
- `twitterCreateTweet` - Post tweet
- `twitterSearchTweets` - Search tweets
- `discordSendMessage` - Send message

---

## ğŸ® UI Features

### Provider Selector
```
Dropdown shows:
â”œâ”€ OpenAI (GPT-4 Turbo)
â”œâ”€ Anthropic (Claude 3.5)
â””â”€ Ollama (Local LLM) â† NEW
```

### Model Field
```
Placeholder suggestions:
â”œâ”€ OpenAI: gpt-4-turbo, gpt-4, gpt-3.5-turbo
â”œâ”€ Anthropic: claude-3-5-sonnet, claude-3-opus
â””â”€ Ollama: mistral, neural-chat, llama2, dolphin-mixtral
```

### Base URL
```
Smart defaults:
â”œâ”€ OpenAI: https://api.openai.com/v1
â”œâ”€ Anthropic: https://api.anthropic.com
â””â”€ Ollama: http://localhost:11434
```

### Auth Section
```
OpenAI/Anthropic:
â”œâ”€ Use Saved Credential
â””â”€ Enter API Key

Ollama:
â””â”€ (Hidden - no API key needed!)
```

---

## ğŸ’° Cost Comparison

| Provider | Cost | Privacy | Setup |
|----------|------|---------|-------|
| OpenAI | $$ per 1M tokens | Remote | 2 min |
| Anthropic | $ per 1M tokens | Remote | 2 min |
| **Ollama** | **$0** | **Local** | **5 min** |

---

## ğŸ“ˆ Performance Tips

### Make It Faster âš¡
```json
{
  "model": "mistral",
  "temperature": 0,
  "maxTokens": 512,
  "allowedSteps": ["navigate", "extract"]
}
```

### Make It Better â­
```json
{
  "model": "neural-chat",
  "temperature": 0.1,
  "maxTokens": 2048,
  "allowedSteps": ["navigate", "extract", "click", "apiCall"]
}
```

### Make It Perfect ğŸ¯
```json
{
  "model": "dolphin-mixtral",
  "temperature": 0.2,
  "maxTokens": 3000,
  "allowedSteps": ["navigate", "click", "type", "extract", "apiCall", "screenshot"],
  "timeoutMs": 60000
}
```

---

## ğŸ” Security Notes

**Ollama (Local)**:
- âœ… All data stays on your machine
- âœ… No API keys sent anywhere
- âœ… Runs entirely offline
- âœ… Perfect for sensitive data

**Cloud Providers**:
- âš ï¸ Data sent to API
- âš ï¸ Requires API keys
- âš ï¸ Internet required
- âš ï¸ Potential compliance issues

---

## ğŸ“ Full Configuration Options

```json
{
  // Required
  "type": "aiAgent",
  "provider": "ollama",  // "openai" | "anthropic" | "ollama"
  "model": "mistral",    // Any Ollama model
  "goal": "...",         // What to accomplish

  // Optional - Defaults shown
  "temperature": 0,                              // 0-1, lower = deterministic
  "maxTokens": 2048,                            // 256-8192
  "baseUrl": "http://localhost:11434",          // Custom endpoint
  "timeoutMs": 30000,                           // ms before timeout
  "systemPrompt": "You are an AI agent...",    // Custom instructions
  "allowedSteps": ["navigate", "extract"],     // Available tools
  "storeKey": "agentResult"                    // Variable to save result
}
```

---

## ğŸ”— Resource Links

- **Ollama**: https://ollama.ai
- **Models**: https://ollama.ai/library
- **Docker**: `docker pull ollama/ollama`
- **Documentation**: See docs/OLLAMA_AGENTIC_GUIDE.md

---

## âœ… Verification Checklist

Before going to production:

- [ ] Ollama running: `ollama serve`
- [ ] Model pulled: `ollama list` shows your model
- [ ] Network accessible: `curl http://localhost:11434/api/status`
- [ ] Test simple goal: "Navigate to google.com"
- [ ] Check tool calls in debug log
- [ ] Verify result stored in variable
- [ ] Review execution time
- [ ] Test with real automation goal

---

## ğŸ“ Learning Resources

### Beginner
1. Follow quick start above
2. Test with simple goal
3. Try scraping example

### Intermediate
4. Add more tools to allowedSteps
5. Test with API calls
6. Try multiple iterations

### Advanced
7. Tune temperature and tokens
8. Use custom system prompt
9. Deploy to production
10. Monitor and optimize

---

## ğŸ“ Support

**Common Issues**:
- See Troubleshooting section above
- Check docs/OLLAMA_AGENTIC_GUIDE.md
- Enable debug mode to see tool calls

**Advanced Help**:
- Full guide: docs/OLLAMA_AGENTIC_GUIDE.md
- Verification: docs/OLLAMA_AGENTIC_VERIFICATION.md
- Complete: docs/OLLAMA_AGENTIC_COMPLETE.md

---

## ğŸ‰ You're Ready!

1. âœ… Install Ollama: `ollama serve`
2. âœ… Pull model: `ollama pull mistral`
3. âœ… Create agent in Loopi
4. âœ… Set goal
5. âœ… Execute!

**That's it! You now have local, private, free AI automation!**

---

**Version**: 1.0  
**Last Updated**: January 10, 2026  
**Status**: Production Ready âœ…
